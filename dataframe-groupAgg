# Get count by status from orders

orders.groupBy('order_status').count().show()
orders.groupBy('order_status'). \
  agg(count('order_status').alias('status_count')). \
  show()

# Get revenue for each order id from order items 

orderItems.groupBy('order_item_order_id'). \
  sum('order_item_subtotal'). \
  show()

from pyspark.sql.functions import round, sum
orderItems.groupBy('order_item_order_id'). \
  agg(round(sum('order_item_subtotal'), 2).alias('order_revenue')). \
  show()

# Get daily product revenue 
# filter for complete and closed orders
# groupBy order_date and order_item_product_id
# Use agg and sum on order_item_subtotal to get revenue

spark.conf.set('spark.sql.shuffle.partitions', '2')

from pyspark.sql.functions import sum, round
orders.where('order_status in ("COMPLETE", "CLOSED")'). \
  join(orderItems, orders.order_id == orderItems.order_item_order_id). \
  groupBy('order_date', 'order_item_product_id'). \
  agg(round(sum('order_item_subtotal'), 2).alias('revenue')). \
  show()

orders.where('order_status in ("COMPLETE", "CLOSED")'). \
  join(orderItems, orders.order_id == orderItems.order_item_order_id). \
  groupBy(orders.order_date, orderItems.order_item_product_id). \
  agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue')). \
  show()