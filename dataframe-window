# window functions rank(), dense_rank(), row_number(), lead

orderItemsCSV = spark.read. \
  csv('/public/retail_db/order_items'). \
  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', 
       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')

from pyspark.sql.types import IntegerType, FloatType

orderItems = orderItemsCSV.\
    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \
    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \
    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \
    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \
    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \
    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))


from pyspark.sql.window import Window
spec = Window.partitionBy('order_item_order_id')

from pyspark.sql.functions import sum, round
orderItems. \
  withColumn('order_revenue', round(sum('order_item_subtotal').over(spec), 2)). \
  show()

from pyspark.sql.window import Window
spec = Window. \
  partitionBy('order_item_order_id'). \
  orderBy(orderItems.order_item_subtotal.desc())

from pyspark.sql.functions import rank
orderItems. \
  withColumn('rnk', rank().over(spec)). \
  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \
  show()

from pyspark.sql.window import Window
spec = Window. \
  partitionBy('order_item_order_id'). \
  orderBy(orderItems.order_item_subtotal.desc())

spark.conf.set('spark.sql.shuffle.partitions', '2')
from pyspark.sql.functions import lead
orderItems. \
  withColumn('next_order_item_subtotal', lead('order_item_subtotal').over(spec)). \
  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \
  show()