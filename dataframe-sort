# Sort orders by status

orders.sort('order_status').show()
orders.orderBy('order_status').show()
orders.orderBy(orders.order_status).show()

#Sort orders by date and then by status

orders.sort('order_date', 'order_status').show()
orders.orderBy(orders.order_date, orders.order_status).show()

# Sort order items by order_item_order_id and order_item_subtotal descending

orderItems. \
  sort(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \
  show()

orderItems. \
  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \
  show()

# Take daily product revenue data and 
# sort in ascending order by date and 
# then descending order by revenue.

spark.conf.set('spark.sql.shuffle.partitions', '2')

from pyspark.sql.functions import sum, round

dailyProductRevenue = orders.where('order_status in ("COMPLETE", "CLOSED")'). \
  join(orderItems, orders.order_id == orderItems.order_item_order_id). \
  groupBy(orders.order_date, orderItems.order_item_product_id). \
  agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue'))

dailyProductRevenueSorted = dailyProductRevenue. \
  orderBy(dailyProductRevenue.order_date, dailyProductRevenue.revenue.desc())

dailyProductRevenueSorted.show()